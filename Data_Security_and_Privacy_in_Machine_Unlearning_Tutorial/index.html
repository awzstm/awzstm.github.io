<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ICDM 2025 Tutorial | Security and Privacy in Machine Unlearning</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Add scroll margin to account for sticky navigation. Adjust if your sidebar navigation changes top position. */
        section[id] {
            scroll-margin-top: 2rem; 
        }

        /* Custom style for the hero title background */
        .hero-title-bg {
            background-image: url('https://dcist.com/wp-content/uploads/sites/3/2021/02/The-Yards-dusk-1500x1000.jpg'); /* Replace with your actual DC at night image URL */
            background-size: cover;
            background-position: 0% 5%; /* Adjust to focus on the desired part of the image */ 
            color: white; /* Ensure text is readable against the dark background */
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); /* Add text shadow for better readability */
        }
        .inbox-text {
            margin-left: 2.5rem;
            margin-right: 2.5rem;
        }
    </style>
    <!-- https://images.foxtv.com/static.fox5dc.com/www.fox5dc.com/content/uploads/2023/09/1280/720/DC_NASA_File.jpg?ve=1&tl=1 -->
<!-- </head>https://cloudinary.fclmedia.com/fctg/image/fetch/h_660,w_1200,q_100,c_fill,g_auto,fl_progressive/https://content.flightcentre.com/sites/default/files/gb/washington-dc-hero-ps.jpg -->
 <!-- https://vastphotos.com/files/uploads/photos/11330/washington-dc-mall-sunrise-l.jpg?v=20221018091735 -->
<body class="bg-gray-100 text-gray-800">


    <div class="hero-title-bg text-center py-16 md:py-24 mb-8 md:mb-12">
        <h1 class="text-6xl md:text-6xl font-bold leading-tight px-4">
            Data Security and Privacy in Machine Unlearning: 
        </h1>
        <p class="text-2xl md:text-4xl font-semibold mt-4 px-4">
            Recent Advances, Challenges, and Future Perspectives
        </p>
        <p class="text-lg md:text-xl font-medium mt-6 px-4">
            ICDM 2025 Tutorial | Washington DC, USA
        </p>
        <p class="text-lg md:text-xl font-medium mt-2 px-4">
            Aobo Chen, Wei Qian, Zheyuan Liu, Shagufta Mehnaz, Tianhao Wang, Mengdi Huai
        </p>
        <p class="text-md md:text-lg font-normal mt-2 px-4">
            Tutorial Date: TBD
        </p>
    </div>
    <!-- <div class="hero-title-bg py-16 md:py-24 mb-8 md:mb-12">
        <div class="container mx-auto text-left">
            <h1 class="text-6xl md:text-6xl font-bold leading-tight px-4">
                Data Security and Privacy in Machine Unlearning: 
            </h1>
            <p class="text-2xl md:text-4xl font-semibold mt-4 px-4">
                Recent Advances, Challenges, and Future Perspectives
            </p>
            <p class="text-lg md:text-xl font-medium mt-6 px-4">
                ICDM 2025 Tutorial | Washington DC, USA
            </p>
            <p class="text-lg md:text-xl font-medium mt-2 px-4">
                Aobo Chen, Wei Qian, Zheyuan Liu, Shagufta Mehnaz, Tianhao Wang, Mengdi Huai
            </p>
            <p class="text-md md:text-lg font-normal mt-2 px-4">
                Sunday, August 3, 2025 | 8:00 AM - 11:00 AM (Schedule placeholder)
            </p>
        </div>
    </div> -->
    <div class="container mx-auto flex flex-col md:flex-row py-8 px-4 md:space-x-8">

        <aside class="w-full md:w-1/4 lg:w-1/5 mb-8 md:mb-0">
            <div class="sticky top-4">
                <div class="p-4 bg-white rounded-lg shadow-md">
                    <h3 class="font-bold text-lg mb-3 border-b pb-2">Navigation</h3>
                    <nav class="flex flex-col space-y-2">
                        <a href="#abstract" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Tutorial Abstract</a>
                        <a href="#audience" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Target Audience</a>
                        <a href="#materials" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Tutorial Materials</a>
                        <a href="#modules" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Tutorial Modules</a>
                        <a href="#related-tutorials" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Related Tutorials</a>
                        <a href="#contributors" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Contributors</a>
                        <a href="#contact" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Contact</a>
                        <a href="#acknowledgments" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">Acknowledgments</a>
                        <a href="#references" class="text-gray-700 hover:text-blue-600 hover:font-semibold transition duration-200">References</a>
                    </nav>
                </div>
            </div>
        </aside>

        <main class="w-full md:w-3/4 lg:w-4/5 bg-white p-6 md:p-8 rounded-lg shadow-md">
            
            <div class="space-y-16">
                <section id="abstract">
                    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Tutorial Abstract</h2>
                    <p class="text-gray-700 leading-relaxed inbox-text">
                        Machine unlearning enables the removal of specific data or knowledge from trained models without retraining from scratch, thereby operationalizing the privacy principle of the right to be forgotten. It has gained significant attention in recent years. However, the development of machine unlearning is associated with inherent vulnerabilities and threats, posing significant security and privacy challenges for researchers and practitioners. This tutorial will focus on the following aspects: (1) providing a comprehensive review of security and privacy challenges in machine unlearning from the data mining perspective; (2) introducing cutting-edge techniques to solve the security and privacy risks in machine unlearning from both data and model perspectives; and (3) identifying open challenges and proposing convincing future research directions in robust unlearning. We believe this is an emerging and potentially high-impact topic, which will attract both researchers and practitioners from academia and industry.
                    </p>
                </section>
                <section id="audience">
                    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Target Audience and Prerequisites</h2>
                    <p class="text-gray-700 leading-relaxed inbox-text">The participants are expected to have basic knowledge of linear algebra, data mining, and machine learning. Those audience with experience in fields such as security, adversarial robustness, privacy, machine unlearning, foundation models, and large language models are especially encouraged to participate in Q&A sessions during the tutorial or engage in offline discussions. Participants will gain insights into preserving data privacy and security while harnessing the power of machine unlearning, a crucial skill for enforcing “the right to be forgotten”. We expect the number of participants for this tutorial to be around 30.</p>     
                </section>

                <section id="materials">
                    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Tutorial Materials</h2>
                    <p class="text-gray-700 leading-relaxed mb-8 text-center">All materials for this tutorial, including slides and a video recording, will be made available here.</p>
                    <div class="flex justify-center space-x-4">
                        <a href="#" class="bg-blue-600 text-white font-semibold px-8 py-3 rounded-lg shadow-lg hover:bg-blue-700 transition duration-300 transform hover:scale-105">Our Slides</a>
                        <a href="#" class="bg-green-600 text-white font-semibold px-8 py-3 rounded-lg shadow-lg hover:bg-gray-700 transition duration-300 transform hover:scale-105">Our Video</a>
                    </div>
                </section>

                <section id="modules">
                    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Tutorial Modules</h2>
                    <div class="space-y-1">
                        <p class="leading-relaxed inbox-text">• <strong>Part 1:</strong> Introduction (10 minutes)</p>
                        <p class="leading-relaxed inbox-text">• <strong>Part 2:</strong> Security and Privacy (20 minutes)</p>
                        <p class="leading-relaxed inbox-text">• <strong>Part 3:</strong> Security Attacks and Countermeasures in Machine Unlearning (30 minutes)</p>
                        <p class="leading-relaxed inbox-text">• <strong>Part 4:</strong> Privacy Attacks and Countermeasures in Machine Unlearning (30 minutes)</p>
                        <p class="leading-relaxed inbox-text">• <strong>Part 5:</strong> Summary and Future Work (10 minutes)</p>
                    </div>
                </section>


<section id="related-tutorials">
    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Related Tutorials</h2>

            <p class="leading-relaxed inbox-text">
                • <a href="https://sites.google.com/view/cvpr-2024-tutorial" class="text-blue-500 hover:underline" target="_blank">S. Liu, Y. Liu, N. B. Angel, and E. Triantafillou, “Machine unlearning in computer vision: Foundations and applications (cvpr 2024 tutorial),” in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.</a>
            </p>
            <p class="leading-relaxed inbox-text">
                
              
                • <a href="https://eprint.iacr.org/2025/724" class="text-blue-500 hover:underline" target="_blank">D. Alabi, S. Galhotra, S. Mehnaz, Z. Song, and E. Wu, “Privacy and security in distributed data markets,” in Companion of the 2025 International Conference on Management of Data, 2025, pp. 775–787.</a>
            </p>

            <p class="leading-relaxed inbox-text">

                • <a href="https://labrai.github.io/KDD2025_Tutorial/" class="text-blue-500 hover:underline" target="_blank">L. Li, K. Zhao, K. Ding, Y. Zhao, Y. Dong, and N. Gong, “Model extraction attack and defense for large language models: Recent advances, challenges, and future perspectives (kdd 2025 tutorial),” in Proceedings of the 31th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2025.</a>
            </p>

</section>

 <section id="contributors">
    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Contributors</h2>
    <div class="space-y-8">
        <div class="flex flex-col sm:flex-row items-start sm:space-x-6">
            <img src="https://placehold.co/200x200/E2E8F0/4A5568?text=AC" alt="Aobo Chen" class="w-24 h-24 sm:w-32 sm:h-32 rounded-full flex-shrink-0 mb-4 sm:mb-0">
            <div>
                <h3 class="text-xl font-bold mb-2">Aobo Chen</h3>
                <p class="text-gray-700 leading-relaxed">Aobo Chen is currently
a Ph.D. student in the Department of Computer Science at
the Iowa State University (ISU). His research focuses on data
mining and machine learning, with particular emphasis on the
security and privacy vulnerabilities of modern models. His
recent work investigates adversarial risks, machine unlearning,
privacy leakage, and robustness challenges in trustworthy AI.
He has been recognized with the Publication Awards in 2024
and 2025 from the Computer Science department at ISU for his
research contributions and continues to advance his research.</p>
            </div>
        </div>
        <div class="flex flex-col sm:flex-row items-start sm:space-x-6">
            <img src="https://placehold.co/200x200/E2E8F0/4A5568?text=WQ" alt="Wei Qian" class="w-24 h-24 sm:w-32 sm:h-32 rounded-full flex-shrink-0 mb-4 sm:mb-0">
            <div>
                <h3 class="text-xl font-bold mb-2">Wei Qian</h3>
                <p class="text-gray-700 leading-relaxed">Wei Qian is currently a Ph.D.
student in the Department of Computer Science at ISU. His
research interests are data mining and machine learning. More
specifically, he is working on exploring adversarial robustness,
security and privacy in machine unlearning, and trustworthy AI.
He has also earned multiple awards, including the Publication
Awards from the Computer Science department at ISU in
2024 and 2025, and the Graduate College Research Excellence
Award at the Iowa State University.</p>
            </div>
        </div>
        <div class="flex flex-col sm:flex-row items-start sm:space-x-6">
            <img src="https://placehold.co/200x200/E2E8F0/4A5568?text=ZL" alt="Zheyuan Liu" class="w-24 h-24 sm:w-32 sm:h-32 rounded-full flex-shrink-0 mb-4 sm:mb-0">
            <div>
                <h3 class="text-xl font-bold mb-2">Zheyuan Liu</h3>
                <p class="text-gray-700 leading-relaxed">Zheyuan Liu is a Ph.D. student
in Computer Science and Engineering at the University of Notre Dame. His research interests lie in post-training techniques,
generative AI, and agentic AI. In particular, he focuses on
developing methods such as machine unlearning to enhance
the trustworthiness of GenAIs, including their privacy, safety,
and fairness. He has received multiple awards and fellowships,
including the Professional Development Award from the
Department of Computer Science and Engineering at Notre
Dame in 2024. One of his most recent unlearning works has
obtained a U.S. patent.</p>
            </div>
        </div>
        <div class="flex flex-col sm:flex-row items-start sm:space-x-6">
            <img src="https://placehold.co/200x200/E2E8F0/4A5568?text=SM" alt="Shagufta Mehnaz" class="w-24 h-24 sm:w-32 sm:h-32 rounded-full flex-shrink-0 mb-4 sm:mb-0">
            <div>
                <h3 class="text-xl font-bold mb-2">Shagufta Mehnaz</h3>
                <p class="text-gray-700 leading-relaxed">Dr. Shagufta Mehnaz is an
assistant professor in the Department of Computer Science
and Engineering at the Pennsylvania State University. Her
research interests are at the intersection of security, privacy,
and machine learning, with a focus on topics such as secure
and privacy-preserving machine unlearning. Her research has
been published in top-tier Security/Privacy and ML/AI/data
mining conferences. She has earned multiple awards, including
the NSF CAREER Award, the Best Paper Award in ACM
CODASPY 2017, the Bilsland Dissertation Fellowship (Purdue
University), and the Faculty For The Future Fellowship.</p>
            </div>
        </div>
        <div class="flex flex-col sm:flex-row items-start sm:space-x-6">
            <img src="https://placehold.co/200x200/E2E8F0/4A5568?text=TW" alt="Tianhao Wang" class="w-24 h-24 sm:w-32 sm:h-32 rounded-full flex-shrink-0 mb-4 sm:mb-0">
            <div>
                <h3 class="text-xl font-bold mb-2">Tianhao Wang</h3>
                <p class="text-gray-700 leading-relaxed">Dr. Tianhao Wang is a
data privacy and security researcher that joined the University
of Virginia (UVA) in 2022. He serves as an assistant professor
in the Department of Computer Science and at the School of
Data Science by Courtesy. He has extensive publications in top
security and database conferences. His work about differentially
private synthetic data generation won multiple awards in NIST's
competition. Prior to the University of Virginia, he obtained
his Ph.D. from Purdue University in 2021 and held a postdoc
position at Carnegie Mellon University.</p>
            </div>
        </div>
        <div class="flex flex-col sm:flex-row items-start sm:space-x-6">
            <img src="https://placehold.co/200x200/E2E8F0/4A5568?text=MH" alt="Mengdi Huai" class="w-24 h-24 sm:w-32 sm:h-32 rounded-full flex-shrink-0 mb-4 sm:mb-0">
            <div>
                <h3 class="text-xl font-bold mb-2">Mengdi Huai</h3>
                <p class="text-gray-700 leading-relaxed">Dr. Mengdi Huai is currently an assistant professor in the Department of Computer
Science at ISU. Her research interests lie in data mining and
machine learning, with the current emphasis on security and
robustness, machine unlearning, and privacy protection. She
has received multiple awards, including the NSF CAREER
Award, the AAAI New Faculty Highlights, the Departmental
Excellence Award (ISU), the Rising Star in EECS at MIT, the
Sture G. Olsson Fellowship in Engineering (UVA), and the
Best Paper Runner-up for KDD 2020.</p>
            </div>
        </div>
    </div>
</section>
<section id="contact">
    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Contact</h2>
    <p class="text-gray-700 leading-relaxed inbox-text">
        For questions regarding the tutorial, please contact us at <a href="mailto:mdhuai@iastate.edu" class="text-blue-500 underline">mdhuai@iastate.edu</a>.
    </p>
</section>
<section id="acknowledgments">
    <h2 class="text-2xl font-bold mb-6 border-b pb-3">Acknowledgments</h2>
    <p class="text-gray-700 leading-relaxed inbox-text">
        We sincerely thank all the contributors and the community for their continuous support and valuable feedback. This research is funded by NSF Grant #2350332, "Security and Privacy in Machine Unlearning", and NSF Grant #2442750, "CAREER: Enabling Reliable Uncertainty-Aware Decision Making with Unreliable Data".
    </p>
</section>
<section id="references">
    <h2 class="text-2xl font-bold mb-6 border-b pb-3">References</h2>
    <div class="space-y-4 text-gray-700 text-sm">
        <p class="leading-relaxed">[1] A. Chen, Y. Li, C. Zhao, and M. Huai, “A survey of security and privacy issues of machine unlearning,” 2025.</p>
        <p class="leading-relaxed">[2] S. Liu, Y. Liu, N. B. Angel, and E. Triantafillou, “Machine unlearning in computer vision: Foundations and applications (cvpr 2024 tutorial),” in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.</p>
        <p class="leading-relaxed">[3] V. Patil, M. Mazeika, W. Hodgkins, S. Basart, Y. Liu, K. Lee, M. Bansal, and B. Li, “Machine unlearning for generative ai (icml 2025 workshop),” in Proceedings of the 42st International Conference on Machine Learning (ICML), 2025.</p>
        <p class="leading-relaxed">[4] D. Alabi, S. Galhotra, S. Mehnaz, Z. Song, and E. Wu, “Privacy and security in distributed data markets,” in Companion of the 2025 International Conference on Management of Data, 2025, pp. 775–787.</p>
        <p class="leading-relaxed">[5] L. Li, K. Zhao, K. Ding, Y. Zhao, Y. Dong, and N. Gong, “Model extraction attack and defense for large language models: Recent advances, challenges, and future perspectives (kdd 2025 tutorial),” in Proceedings of the 31th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2025.</p>
        <p class="leading-relaxed">[6] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang, “Graph unlearning,” in Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, 2022.</p>
        <p class="leading-relaxed">[7] C. Gong, K. Li, J. Yao, and T. Wang, “TrajDeleter: Enabling trajectory forgetting in offline reinforcement learning agents,” in NDSS, 2025.</p>
        <p class="leading-relaxed">[8] Z. Liu, G. Dou, E. Chien, C. Zhang, Y. Tian, and Z. Zhu, “Breaking the trilemma of privacy, utility, and efficiency via controllable machine unlearning,” in Proceedings of the ACM Web Conference 2024, 2024, pp. 1260–1271.</p>
        <p class="leading-relaxed">[9] Z. Liu, G. Dou, M. Jia, Z. Tan, Q. Zeng, Y. Yuan, and M. Jiang, “Protecting privacy in multimodal large language models with MLLMU-bench,” in Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Association for Computational Linguistics, 2025, pp. 4105–4135.</p>
        <p class="leading-relaxed">[10] K. Gu, M. R. U. Rashid, N. Sultana, and S. Mehnaz, “Robust unlearning for large language models,” in PAKDD (5), 2025, pp. 143–155.</p>
        <p class="leading-relaxed">[11] W. Qian, C. Zhao, W. Le, M. Ma, and M. Huai, “Towards understanding and enhancing robustness of deep learning models against malicious unlearning attacks,” in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023, pp. 1932–1942.</p>
        <p class="leading-relaxed">[12] C. Zhao, W. Qian, R. Ying, and M. Huai, “Static and sequential malicious attacks in the context of selective forgetting,” Advances in Neural Information Processing Systems, vol. 36, pp. 74 966–74979, 2023.</p>
        <p class="leading-relaxed">[13] W. Qian, A. Chen, C. Zhao, Y. Li, and M. Huai, “Exploring fairness in educational data mining in the context of the right to be forgotten,” arXiv preprint arXiv:2405.16798, 2024.</p>
        <p class="leading-relaxed">[14] Y. Huang, D. Liu, L. Chua, B. Ghazi, P. Kamath, R. Kumar, P. Manurangsi, M. Nasr, A. Sinha, and C. Zhang, “Unlearn and burn: Adversarial machine unlearning requests destroy model accuracy,” arXiv preprint arXiv: 2410.09591, 2024.</p>
        <p class="leading-relaxed">[15] B. Ma, T. Zheng, H. Hu, D. Wang, S. Wang, Z. Ba, Z. Qin, and K. Ren, “Releasing malevolence from benevolence: The menace of benign data on machine unlearning,” arXiv preprint arXiv:2407.05112, 2024.</p>
        <p class="leading-relaxed">[16] J. Z. Di, J. Douglas, J. Acharya, G. Kamath, and A. Sekhari, “Hidden poison: Machine unlearning enables camouflaged poisoning attacks,” in NeurIPS ML Safety Workshop, 2022.</p>
        <p class="leading-relaxed">[17] C. Zhao, W. Qian, Y. Li, A. Chen, and M. Huai, “Rethinking adversarial robustness in the context of the right to be forgotten,” in Proceedings of the 41st International Conference on Machine Learning, 2024, pp. 60927–60939.</p>
        <p class="leading-relaxed">[18] Z. Liu, T. Wang, M. Huai, and C. Miao, “Backdoor attacks via machine unlearning,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, 2024, pp. 14115–14 123.</p>
        <p class="leading-relaxed">[19] Z. Huang, Y. Mao, and S. Zhong, “{UBA-Inf}: Unlearning activated backdoor attack with {Influence-Driven} camouflage,” in 33rd USENIX Security Symposium (USENIX Security 24), 2024, pp. 4211–4228.</p>
        <p class="leading-relaxed">[20] M. Alam, H. Lamri, and M. Maniatakos, “Reveil: Unconstrained concealed backdoor attack on deep neural networks using machine unlearning,” arXiv preprint arXiv: 2502.11687, 2025.</p>
        <p class="leading-relaxed">[21] J. Ji, Y. Liu, Y. Zhang, G. Liu, R. Kompella, S. Liu, and S. Chang, “Reversing the forget-retain objectives: An efficient Ilm unlearning framework from logit difference,” Advances in Neural Information Processing Systems, vol. 37, pp. 12581–12611, 2024.</p>
        <p class="leading-relaxed">[22] Y. Hu, J. Lou, J. Liu, W. Ni, F. Lin, Z. Qin, and K. Ren, “Eraser: Machine unlearning in mlaas via an inference serving-aware approach,” in Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, 2024, pp. 3883–3897.</p>
        <p class="leading-relaxed">[23] M. R. U. Rashid, J. Liu, T. Koike-Akino, Y. Wang, and S. Mehnaz, “Forget to flourish: Leveraging machine-unlearning on pretrained language models for privacy leakage,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, 2025, pp. 20 139–20 147.</p>
        <p class="leading-relaxed">[24] J. Łucki, B. Wei, Y. Huang, P. Henderson, F. Tramèr, and J. Rando, “An adversarial perspective on machine unlearning for ai safety,” Transactions on Machine Learning Research, 2025.</p>
        <p class="leading-relaxed">[25] H. Yuan, Z. Jin, P. Cao, Y. Chen, K. Liu, and J. Zhao, “Towards robust knowledge unlearning: An adversarial framework for assessing and improving unlearning robustness in large language models,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 24, 2025, pp. 25769–25777.</p>
        <p class="leading-relaxed">[26] H. Hu, S. Wang, T. Dong, and M. Xue, “Learn what you want to unlearn: Unlearning inversion attacks against machine unlearning,” in 2024 IEEE Symposium on Security and Privacy (SP). IEEE, 2024. pp. 3257–3275.</p>
        <p class="leading-relaxed">[27] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang, “When machine unlearning jeopardizes privacy,” in Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, 2021, pp. 896–911.</p>
        <p class="leading-relaxed">[28] W. Wang, C. Zhang, Z. Tian, S. Liu, and S. Yu, “Crfu: Compressive representation forgetting against privacy leakage on machine unlearning,” IEEE Transactions on Dependable and Secure Computing, 2025.</p>
        <p class="leading-relaxed">[29] C. Fan, J. Jia, Y. Zhang, A. Ramakrishna, M. Hong, and S. Liu, “Towards Ilm unlearning resilient to relearning attacks: A sharpness-aware minimization perspective and beyond,” International conference on machine learning, 2025.</p>
        <p class="leading-relaxed">[30] B. Zhang, Y. Dong, T. Wang, and J. Li, “Towards certified unlearning for deep neural networks,” in Forty-first International Conference on Machine Learning, 2024.</p>
    </div>
</section>


            </div>
        </main>
    </div>

    <footer class="bg-gray-800 text-white py-6 mt-8">
        <div class="container mx-auto px-6 text-center">
            <p>&copy; 2025 ICDM Tutorial. All rights reserved.</p>
        </div>
    </footer>

</body>
</html>